{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "NER _assignment4.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ID7jogaa1i",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4: Named entity recognition\n",
        "\n",
        "Create a model for Named Entity Recognition for dataset CoNLL 2002.  \n",
        "Your quality metric = f1_macro\n",
        "\n",
        "In your solution you should use: RandomForest, Gradient Boosting (xgboost, lightgbm, catboost)   \n",
        "Tutorials:  \n",
        "1. https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n",
        "1. https://github.com/catboost/tutorials \n",
        "\n",
        "More baselines you beat - better your score\n",
        " \n",
        "baseline 1 [3 points]: 0.0604      random labels  \n",
        "baseline 2 [5 points]: 0.3966      PoS features + logistic regression  \n",
        "baseline 3 [8 points]: 0.8122      word2vec cbow embedding + baseline 2 + svm    \n",
        "\n",
        "[1 point] using feature engineering (creating features not presented in the baselines)\n",
        "\n",
        "! Your results must be reproducible. You should explicitly set all seeds random_states in yout model.  \n",
        "! Remember to use proper training pipeline.  \n",
        "\n",
        "bonus, think about:  \n",
        "1. [1 point] Why did we select f1 score with macro averaging as our classification quality measure? What other metrics are suitable?   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MVfediE958S",
        "colab_type": "text"
      },
      "source": [
        "Мы берем f-score т.к. у нас несбалансированный датасет. Метрики типа Accuracy будет не очень круто использовать, ибо у нас перевес класс O (0.85) от всего датасета. Почему именно f1-score с макро усреднением? Потому что эта метрика позволяет считать f-score для каждого класса независимо, и затем усредняет результат. f-score с микро усреднением будет учитывать вес для каждой оценки класса в зависимости от кол-ва элементов в классе (а нам это не очень нужно, ибо доминирующий класс у нас - O). Еще мы можем использовать AUC, или метрики с макро усреднением."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28hQPI9naa1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "SEED=1338"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KwA4W4Qaa1s",
        "colab_type": "code",
        "outputId": "7df34763-d381-46de-8ea0-d55bf836f01b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "df = pd.read_csv('ner_short.csv', index_col=0)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>NNS</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VBP</td>\n",
              "      <td>have</td>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>IN</td>\n",
              "      <td>NNS</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>1.0</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBP</td>\n",
              "      <td>have</td>\n",
              "      <td>NNS</td>\n",
              "      <td>IN</td>\n",
              "      <td>NNS</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>1.0</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IN</td>\n",
              "      <td>through</td>\n",
              "      <td>VBN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBP</td>\n",
              "      <td>NNS</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>1.0</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NNP</td>\n",
              "      <td>London</td>\n",
              "      <td>IN</td>\n",
              "      <td>through</td>\n",
              "      <td>VBN</td>\n",
              "      <td>VBP</td>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>1.0</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  next-next-pos next-next-word next-pos  ... sentence_idx           word tag\n",
              "0           NNS  demonstrators       IN  ...          1.0      Thousands   O\n",
              "1           VBP           have      NNS  ...          1.0             of   O\n",
              "2           VBN        marched      VBP  ...          1.0  demonstrators   O\n",
              "3            IN        through      VBN  ...          1.0           have   O\n",
              "4           NNP         London       IN  ...          1.0        marched   O\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvoqh1Cgaa1x",
        "colab_type": "code",
        "outputId": "93db0441-9bbf-4b88-83f1-6bc2fc6f5849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# number of sentences\n",
        "df.sentence_idx.max()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1500.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxfbipjoaa10",
        "colab_type": "code",
        "outputId": "8d0ef03a-83b4-4735-ce98-291552ba8536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "# class distribution\n",
        "df.tag.value_counts(normalize=True )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O        0.852828\n",
              "B-geo    0.027604\n",
              "B-gpe    0.020935\n",
              "B-org    0.020247\n",
              "I-per    0.017795\n",
              "B-tim    0.016927\n",
              "B-per    0.015312\n",
              "I-org    0.013937\n",
              "I-geo    0.005383\n",
              "I-tim    0.004247\n",
              "B-art    0.001376\n",
              "I-gpe    0.000837\n",
              "I-art    0.000748\n",
              "B-eve    0.000628\n",
              "I-eve    0.000508\n",
              "B-nat    0.000449\n",
              "I-nat    0.000239\n",
              "Name: tag, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFoNXF5aa16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence length\n",
        "tdf = df.set_index('sentence_idx')\n",
        "tdf['length'] = df.groupby('sentence_idx').tag.count()\n",
        "df = tdf.reset_index(drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fNWkc9faa2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode categorial variables\n",
        "le = LabelEncoder()\n",
        "df['pos'] = le.fit_transform(df.pos)\n",
        "df['next-pos'] = le.fit_transform(df['next-pos'])\n",
        "df['next-next-pos'] = le.fit_transform(df['next-next-pos'])\n",
        "df['prev-pos'] = le.fit_transform(df['prev-pos'])\n",
        "df['prev-prev-pos'] = le.fit_transform(df['prev-prev-pos'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP-jOeEpaa2D",
        "colab_type": "code",
        "outputId": "edacd520-ca89-4a96-a6c0-a425e860f6f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>16</td>\n",
              "      <td>London</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_idx  next-next-pos next-next-word  ...           word tag  length\n",
              "0           1.0             18  demonstrators  ...      Thousands   O      48\n",
              "1           1.0             33           have  ...             of   O      48\n",
              "2           1.0             32        marched  ...  demonstrators   O      48\n",
              "3           1.0              9        through  ...           have   O      48\n",
              "4           1.0             16         London  ...        marched   O      48\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJou53vDaa2G",
        "colab_type": "code",
        "outputId": "2fb3095c-8e12-4cc6-8998-639e9ba5aa7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# splitting\n",
        "y = LabelEncoder().fit_transform(df.tag)\n",
        "\n",
        "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n",
        "                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
        "print('train', df_train.shape[0])\n",
        "print('test', df_test.shape[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 50155\n",
            "test 16719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKdCAAteaa2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some wrappers to work with word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from collections import defaultdict\n",
        "\n",
        "   \n",
        "class Word2VecWrapper(TransformerMixin):\n",
        "    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n",
        "        self.window_ = window\n",
        "        self.negative_ = negative\n",
        "        self.size_ = size\n",
        "        self.iter_ = iter\n",
        "        self.is_cbow_ = is_cbow\n",
        "        self.w2v = None\n",
        "        self.random_state = random_state\n",
        "        \n",
        "    def get_size(self):\n",
        "        return self.size_\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        X: list of strings\n",
        "        \"\"\"\n",
        "        sentences_list = [x.split() for x in X]\n",
        "        self.w2v = Word2Vec(sentences_list, \n",
        "                            window=self.window_,\n",
        "                            negative=self.negative_, \n",
        "                            size=self.size_, \n",
        "                            iter=self.iter_,\n",
        "                            sg=not self.is_cbow_, seed=self.random_state)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def has(self, word):\n",
        "        return word in self.w2v\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        X: a list of words\n",
        "        \"\"\"\n",
        "        if self.w2v is None:\n",
        "            raise Exception('model not fitted')\n",
        "        return np.array([self.w2v[w] if w in self.w2v else np.zeros(self.size_) for w in X ])\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0K61HoYaa2N",
        "colab_type": "code",
        "outputId": "b21fe1b0-907e-4877-900e-4e5d97904e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "# here we exploit that word2vec is an unsupervised learning algorithm\n",
        "# so we can train it on the whole dataset (subject to discussion)\n",
        "\n",
        "sentences_list = [x.strip() for x in ' '.join(df.word).split('.')]\n",
        "\n",
        "w2v_cbow = Word2VecWrapper(window=5, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\n",
        "w2v_cbow.fit(sentences_list)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 41.7 s, sys: 388 ms, total: 42.1 s\n",
            "Wall time: 23.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLK12b7Baa2R",
        "colab_type": "code",
        "outputId": "46d1a3a0-46b8-4af8-c020-fbfdddc6cc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "# baseline 1 \n",
        "# random labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\n",
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('est', DummyClassifier(random_state=SEED)),\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.058085999075392906\n",
            "test 0.05733334797725607\n",
            "CPU times: user 117 ms, sys: 13 ms, total: 130 ms\n",
            "Wall time: 139 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTDNrpzvaa2U",
        "colab_type": "code",
        "outputId": "f7be648c-cadc-46b2-9cd9-24708a728fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "# baseline 2 \n",
        "# pos features + one hot encoding + logistic regression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('est', LogisticRegressionCV(Cs=5, cv=5, n_jobs=-1, scoring='f1_macro', \n",
        "                             penalty='l2', solver='newton-cg', multi_class='multinomial', random_state=SEED)),\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.462228322587577\n",
            "test 0.3869903611637815\n",
            "CPU times: user 21.4 s, sys: 189 ms, total: 21.6 s\n",
            "Wall time: 17min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoftVhLHaa2Y",
        "colab_type": "code",
        "outputId": "0c5666c5-cf7c-49aa-c3ec-dde833c8c8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time\n",
        "# baseline 3\n",
        "# use word2vec cbow embedding + baseline 2 + svm\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.svm import LinearSVC\n",
        "import scipy.sparse as sp\n",
        "\n",
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X_train = sp.hstack([\n",
        "    embeding.transform(df_train.word),\n",
        "    embeding.transform(df_train['next-word']),\n",
        "    embeding.transform(df_train['next-next-word']),\n",
        "    embeding.transform(df_train['prev-word']),\n",
        "    embeding.transform(df_train['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "X_test = sp.hstack([\n",
        "    embeding.transform(df_test.word),\n",
        "    embeding.transform(df_test['next-word']),\n",
        "    embeding.transform(df_test['next-next-word']),\n",
        "    embeding.transform(df_test['prev-word']),\n",
        "    embeding.transform(df_test['prev-prev-word']),\n",
        "    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "model = model_selection.GridSearchCV(LinearSVC(penalty='l2', multi_class='ovr', random_state=SEED), \n",
        "                                    {'C': np.logspace(-4, 0, 5)}, \n",
        "                                    cv=3, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 14.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train 0.954575529712029\n",
            "test 0.8269826514181652\n",
            "CPU times: user 2min 40s, sys: 1.38 s, total: 2min 42s\n",
            "Wall time: 16min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2YkuQb4-QHJ",
        "colab_type": "text"
      },
      "source": [
        "# And now my code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuIbbjlhAGlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO4hVcIJ-SCl",
        "colab_type": "code",
        "outputId": "01fdc864-7df0-4d4c-fd14-0bed81be87cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time \n",
        "#simple OneHotEncoder + RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "\n",
        "#df_train, df_test, y_train, y_test = model_selection.train_test_split(df[columns], y, stratify=y, \n",
        "#\n",
        "#                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
        "hyper_params = { \n",
        "    'n_estimators': [100, 125],\n",
        "    'max_depth' : [12, 18],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'class_weight' :[None, 'balanced']\n",
        "}\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('clf', GridSearchCV(RandomForestClassifier(random_state=SEED), \n",
        "                         param_grid=hyper_params, \n",
        "                         scoring='f1_macro',\n",
        "                         verbose=1,\n",
        "                         n_jobs=-1))\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  2.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train 0.6133386284740426\n",
            "test 0.5158293900073481\n",
            "CPU times: user 9.49 s, sys: 156 ms, total: 9.65 s\n",
            "Wall time: 2min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjcLIQ8P-agy",
        "colab_type": "code",
        "outputId": "5643c8b7-15a4-4934-80b8-55120a9b50c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "best_params = model['clf'].best_params_\n",
        "best_params"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_weight': 'balanced',\n",
              " 'criterion': 'entropy',\n",
              " 'max_depth': 18,\n",
              " 'n_estimators': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvJHQI7f-djC",
        "colab_type": "code",
        "outputId": "1d28e80c-76da-4c1c-8c51-8b8378e661fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "#OneHotEncoder + w2v_cbow + RF\n",
        "import scipy.sparse as sp\n",
        "\n",
        "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n",
        "                                                                     test_size=0.25, random_state=SEED, shuffle=True)\n",
        "\n",
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X_train = sp.hstack([\n",
        "    embeding.transform(df_train.word),\n",
        "    embeding.transform(df_train['next-word']),\n",
        "    embeding.transform(df_train['next-next-word']),\n",
        "    embeding.transform(df_train['prev-word']),\n",
        "    embeding.transform(df_train['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "X_test = sp.hstack([\n",
        "    embeding.transform(df_test.word),\n",
        "    embeding.transform(df_test['next-word']),\n",
        "    embeding.transform(df_test['next-next-word']),\n",
        "    embeding.transform(df_test['prev-word']),\n",
        "    embeding.transform(df_test['prev-prev-word']),\n",
        "    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "#Now I use RF with the params from previous step because with GridSearch it takes a lot of time\n",
        "model = RandomForestClassifier(n_estimators=best_params['n_estimators'], \n",
        "                               max_depth = best_params['max_depth'],\n",
        "                               criterion=best_params['criterion'],\n",
        "                               class_weight = best_params['class_weight'])\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.9569947337927774\n",
            "test 0.8447522167790877\n",
            "CPU times: user 9min 8s, sys: 661 ms, total: 9min 9s\n",
            "Wall time: 9min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIAVC0Jb-kYo",
        "colab_type": "text"
      },
      "source": [
        "## Some new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AhhwBE4-m9L",
        "colab_type": "text"
      },
      "source": [
        "Добавим к каждому токену следующие признаки:\n",
        "\n",
        "1) Тип капитализации (1 - Слово; 2 - слово; 3 - СЛОВО; 4 - остальное) - category\n",
        "\n",
        "2) Наличие/отсутсвие цифр. - bool\n",
        "\n",
        "3) Наличие/отсутствие специальных символов - bool\n",
        "\n",
        "4) Длина в символах - int\n",
        "\n",
        "5)Первое слово в предложении или нет - bool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdmZhZrC-sWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.insert(2, 'capitalisation', 'other')\n",
        "df.insert(3, 'digits',  False)\n",
        "df.insert(4, 'symbols', False)\n",
        "df.insert(5, 'length_word', 0)\n",
        "df.insert(6, 'position', False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMZIpfX5-ix2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "ee93fd38-5b31-4401-b737-258325401efe"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>capitalisation</th>\n",
              "      <th>digits</th>\n",
              "      <th>symbols</th>\n",
              "      <th>length_word</th>\n",
              "      <th>position</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>18</td>\n",
              "      <td>other</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>33</td>\n",
              "      <td>other</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>32</td>\n",
              "      <td>other</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>other</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>16</td>\n",
              "      <td>other</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>London</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_idx  next-next-pos capitalisation  ...           word  tag  length\n",
              "0           1.0             18          other  ...      Thousands    O      48\n",
              "1           1.0             33          other  ...             of    O      48\n",
              "2           1.0             32          other  ...  demonstrators    O      48\n",
              "3           1.0              9          other  ...           have    O      48\n",
              "4           1.0             16          other  ...        marched    O      48\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUK0bg-H-q_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "ae907309-c654-4a6e-c286-3c39904279e3"
      },
      "source": [
        "#Изменим типы нетокорые типы колонок на категориальные.\n",
        "df.astype({'pos': 'category', 'capitalisation':'category', 'tag':'category'}).dtypes"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentence_idx       float64\n",
              "next-next-pos        int64\n",
              "capitalisation    category\n",
              "digits                bool\n",
              "symbols               bool\n",
              "length_word          int64\n",
              "position              bool\n",
              "next-next-word      object\n",
              "next-pos             int64\n",
              "next-word           object\n",
              "pos               category\n",
              "prev-pos             int64\n",
              "prev-prev-pos        int64\n",
              "prev-prev-word      object\n",
              "prev-word           object\n",
              "word                object\n",
              "tag               category\n",
              "length               int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xgopbvX-yeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "def find_digits(word):\n",
        "  '''Функция ищет цифры в токене'''\n",
        "  digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "  if len(set(digits) & set(word)) != 0:\n",
        "      return True \n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def find_cap(word):\n",
        "  '''Функция смотрит тип капитализации в токене'''\n",
        "  if word.istitle():\n",
        "    return 'Capit'\n",
        "  elif word.islower():\n",
        "    return 'capit'\n",
        "  elif word.isupper():\n",
        "    return 'CAPIT'\n",
        "  return 'other'\n",
        "\n",
        "def find_symbols(word):\n",
        "  '''Функция смотрит, есть ли специальные символы в токене'''\n",
        "  if len(set(string.punctuation) & set(word)) == 0:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "  \n",
        "def first_position(sentence_id, word_id): \n",
        "  \"\"\"Функция отпределяет начало твита или нет\"\"\"\n",
        "  if word_id == 0:\n",
        "    return True\n",
        "  elif df.sentence_idx[word_id-1] != sentence_id:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Qroxeh-0cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, row in df.iterrows():\n",
        "  digit = find_digits(row.word)# Есть цифры или нет\n",
        "  df.at[i, 'digits'] = digit\n",
        "    \n",
        "  length = len(row.word) # Длина\n",
        "  df.at[i,'length_word'] = length\n",
        "    \n",
        "  cap = find_cap(row.word) # Капитализация\n",
        "  df.at[i, 'capitalisation'] = cap\n",
        "    \n",
        "  sym = find_symbols(row.word) #Наличие специальных символов\n",
        "  df.at[i, 'symbols'] = sym\n",
        "    \n",
        "  first = first_position(row.sentence_idx, i) #Первый элемент в твите или нет\n",
        "  df.at[i, 'position'] = first"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qNzLpO7-2FH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "a5e1a518-e24a-40f4-9939-de739fbe648e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>capitalisation</th>\n",
              "      <th>digits</th>\n",
              "      <th>symbols</th>\n",
              "      <th>length_word</th>\n",
              "      <th>position</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>18</td>\n",
              "      <td>Capit</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "      <td>True</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>33</td>\n",
              "      <td>capit</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>32</td>\n",
              "      <td>capit</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>False</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>capit</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>16</td>\n",
              "      <td>capit</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>7</td>\n",
              "      <td>False</td>\n",
              "      <td>London</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_idx  next-next-pos capitalisation  ...           word  tag  length\n",
              "0           1.0             18          Capit  ...      Thousands    O      48\n",
              "1           1.0             33          capit  ...             of    O      48\n",
              "2           1.0             32          capit  ...  demonstrators    O      48\n",
              "3           1.0              9          capit  ...           have    O      48\n",
              "4           1.0             16          capit  ...        marched    O      48\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtWCvGy8-6MQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "930ee38d-7da1-4663-bef4-403a1b9139b4"
      },
      "source": [
        "%%time\n",
        "# OneHotEncoder + POS features + my features + w2v_cbow + RandomForest with best_params\n",
        "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n",
        "                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X_train = sp.hstack([\n",
        "    embeding.transform(df_train.word),\n",
        "    embeding.transform(df_train['next-word']),\n",
        "    embeding.transform(df_train['next-next-word']),\n",
        "    embeding.transform(df_train['prev-word']),\n",
        "    embeding.transform(df_train['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df_train[['capitalisation', 'pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "X_test = sp.hstack([\n",
        "    embeding.transform(df_test.word),\n",
        "    embeding.transform(df_test['next-word']),\n",
        "    embeding.transform(df_test['next-next-word']),\n",
        "    embeding.transform(df_test['prev-word']),\n",
        "    embeding.transform(df_test['prev-prev-word']),\n",
        "    encoder_pos.transform(df_test[['capitalisation', 'pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=best_params['n_estimators'], \n",
        "                               max_depth = best_params['max_depth'],\n",
        "                               criterion=best_params['criterion'],\n",
        "                               class_weight = best_params['class_weight'])\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.977223528588559\n",
            "test 0.8589386985821288\n",
            "CPU times: user 9min 2s, sys: 239 ms, total: 9min 3s\n",
            "Wall time: 9min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIX_G82kFCHt",
        "colab_type": "text"
      },
      "source": [
        "Бейзлайны побиты!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ptNhzA9E6TJ",
        "colab_type": "text"
      },
      "source": [
        "А теперь попробуем еще раз с GridSearchCV (потому что захотелось)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qwXAqHfuJEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_pipline(model, X, y, hyper_params, SEED=SEED): #This is from my homework2\n",
        "  '''\n",
        "  Функция выполняет пайплайн с семинара: \n",
        "  обучение без регуляризации, затем с регуляризацией на валидационном датасете.\n",
        "  Через GridSearchCV подбирается лучший вариант модели.\n",
        "  \n",
        "  Возвращает список f1_macro score: \n",
        "  до регуляризации, лучшая оценка на валидации, оценка на тестовых данных\n",
        "  Возвращает лучшие параметры модели и лучшую модель\n",
        "  '''\n",
        "  f1_scores = []\n",
        "\n",
        "  X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, \n",
        "                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "  f1_scores.append(metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "\n",
        "  print('Simple fit done.')\n",
        "  grid_search = GridSearchCV(model, cv=3, param_grid=hyper_params, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  print('GridSearchCV done.')\n",
        "  f1_scores.append(grid_search.best_score_)\n",
        "  \n",
        "  model = grid_search.best_estimator_\n",
        "  f1_scores.append(metrics.f1_score(y_test, model.predict(X_test), average='macro'))\n",
        "\n",
        "  best_parms = grid_search.best_params_\n",
        "\n",
        "  return f1_scores, best_parms, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izSvdwBMv5jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X = sp.hstack([\n",
        "    embeding.transform(df.word),\n",
        "    embeding.transform(df['next-word']),\n",
        "    embeding.transform(df['next-next-word']),\n",
        "    embeding.transform(df['prev-word']),\n",
        "    embeding.transform(df['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df[['capitalisation', 'pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj8Ahw5KwRJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "17c0f16f-53bd-4ee9-df9c-7e1cd370708a"
      },
      "source": [
        "%%time\n",
        "#Если что, у меня эта ячейка запускалась час :)\n",
        "hyper_params = {'n_estimators' : [100, 300]} #I don't use more params because it is time consuming\n",
        "scores, best_prms, best_estimator = model_pipline(RandomForestClassifier(), X=X, y=y, hyper_params=hyper_params, SEED=SEED)\n",
        "print('Train f1_score {}; valid f1_score {}; test f1_score {}'.format(scores[0], scores[1], scores[2]))\n",
        "print('Best hyperparams: {}'.format(best_prms))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple fit done.\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 45.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GridSearchCV done.\n",
            "Train f1_score 0.9905000160691169; valid f1_score 0.7759169058979338; test f1_score 0.9091273092985467\n",
            "Best hyperparams: {'n_estimators': 300}\n",
            "CPU times: user 27min 42s, sys: 1.72 s, total: 27min 43s\n",
            "Wall time: 1h 13min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}