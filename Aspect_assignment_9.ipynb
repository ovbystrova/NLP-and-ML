{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Aspect_assignment_9.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mokaem4OdnJ1",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 9\n",
        "\n",
        "Use data from `https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip`  \n",
        "Implement model in pytorch from \"An Unsupervised Neural Attention Model for Aspect Extraction, He et al, 2017\", also desribed in seminar notes.  \n",
        "\n",
        "You can use sentence embeddings with attention **[7 points]**:  \n",
        "$z_s = \\sum_{i}^n \\alpha_i e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$\\alpha_i = softmax(d_i)$  attention weight for i-th token  \n",
        "$d_i = e_{w_i}^T M y_s$ attention with trainable matrix $M \\in R^{dxd}$  \n",
        "$y_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, y_s \\in R^d$ sentence context  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        "\n",
        "**Or** just use sentence embedding as an average over word embeddings **[5 points]**:  \n",
        "$z_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        " \n",
        "$p_t = softmax(W z_s + b), p_t \\in R^K$ topic weights for sentence $s$, with trainable matrix $W \\in R^{dxK}$ and bias vector $b \\in R^K$  \n",
        "$r_s = T^T p_t, r_s \\in R^d$ reconstructed sentence embedding as a weighted sum of topic embeddings   \n",
        "$T \\in R^{Kxd}$ trainable matrix of topic embeddings, K=number of topics\n",
        "\n",
        "\n",
        "**Training objective**:\n",
        "$$ J = \\sum_{s \\in D} \\sum_{i=1}^n max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
        "where   \n",
        "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
        "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
        "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
        "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm\n",
        "\n",
        "\n",
        "**[3 points]** Compute topic coherence for at least for 3 different number of topics. Use 10 nearest words for each topic. It means you have to train one model for each number of topics. You can use code from seminar notes with word2vec similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IwUDMDqpz13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO Заменить texts[23] на что-то более разумное при выборе негативных экземпляров\n",
        "# TODO Заменить в TabularDataset, чтобы neg_{} зависило от NEG_SAMPLES\n",
        "# TODO Сделать в модели так, чтобы neg не было фиксированным, а зависело от NEG_SAMPLES\n",
        "# TODO Определить и проверить структуру Лосса."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qmDndFbY1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6fd107cb-f659-479d-fbc3-7c8d8e5e6b70"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "import spacy \n",
        "\n",
        "import torch\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "from torchtext.vocab import Vectors\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "nltk.download('punkt')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKAfiuLxbGFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "NEG_SAMPLES = 3  # number of negative samples\n",
        "random_state = 23\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKNWvW7zd2Is",
        "colab_type": "text"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqUNSQFBKhrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "62d7b990-2e32-42d8-a6a7-ec889c735b74"
      },
      "source": [
        "!wget -O data.zip https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true\n",
        "!unzip '/content/data.zip'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-20 20:03:47--  https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true\n",
            "Resolving github.com (github.com)... 52.74.223.119\n",
            "Connecting to github.com (github.com)|52.74.223.119|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-20 20:03:47--  https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-20 20:03:48--  https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9927168 (9.5M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   9.47M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-03-20 20:03:51 (135 MB/s) - ‘data.zip’ saved [9927168/9927168]\n",
            "\n",
            "Archive:  /content/data.zip\n",
            "  inflating: data.txt                \n",
            "  inflating: stopwords.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa2LqXnxeFDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/data.txt', 'r') as f:\n",
        "    data = nltk.tokenize.sent_tokenize(f.read())\n",
        "\n",
        "with open('/content/stopwords.txt', 'r') as f:\n",
        "    stopwords = f.read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiKbiGGfoik",
        "colab_type": "code",
        "outputId": "b0fdbc2b-27d0-4ce5-ce6f-25616872ab4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(len(data), data[0])\n",
        "print(len(stopwords), stopwords[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "183400 Barclays' defiance of US fines has merit Barclays disgraced itself in many ways during the pre-financial crisis boom years.\n",
            "350 a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV9X4G4kT5hm",
        "colab_type": "text"
      },
      "source": [
        "# Making DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdWOnwveQcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_df(texts, neg_samples):\n",
        "    \"\"\"\n",
        "    Creating pandas DataFrame from texts and adding randomly chosen negative samples\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame()\n",
        "    df['text'] = texts\n",
        "\n",
        "    for i in range(1, neg_samples+1):\n",
        "        df['neg_{}'.format(i)] = [texts[ind] if ind != el else texts[23] for el, ind in enumerate(np.random.choice(np.arange(0,len(texts)), size=len(texts)))]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEE6p8yXe9m7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = create_df(data, neg_samples=NEG_SAMPLES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QuhOhXQfIys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f1f3ffe4-6f1e-4236-9f3a-d2744b32acaa"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>neg_1</th>\n",
              "      <th>neg_2</th>\n",
              "      <th>neg_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Barclays' defiance of US fines has merit Barcl...</td>\n",
              "      <td>He asks her to be in his music video and, afte...</td>\n",
              "      <td>Most high-profile sites and services have two-...</td>\n",
              "      <td>It shows that too many patients are no longer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So it is tempting to think the bank, when aske...</td>\n",
              "      <td>But at recent rallies Trump has continued to t...</td>\n",
              "      <td>“We want a president to make data-based decisi...</td>\n",
              "      <td>Mace Windu’s is purple because, the exhibition...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>That is not the view of the chief executive, J...</td>\n",
              "      <td>Congressman Filemon Vela, a Texas Democratic, ...</td>\n",
              "      <td>“I told you to get the house cleaned!” She yel...</td>\n",
              "      <td>(A Labor favourite) Why does it take so long t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Barclays thinks the DoJ’s claims are “disconne...</td>\n",
              "      <td>The next three games (West Brom and Leicester ...</td>\n",
              "      <td>It’s really intriguing.</td>\n",
              "      <td>Alternatively, you could buy books while suppo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But actually, some grudging respect for Staley...</td>\n",
              "      <td>80 mins: Bony sums up a poor day, struggling t...</td>\n",
              "      <td>Unlike most other smart home systems, a single...</td>\n",
              "      <td>The 29-year-old actor plays a pickpocket who b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                              neg_3\n",
              "0  Barclays' defiance of US fines has merit Barcl...  ...  It shows that too many patients are no longer ...\n",
              "1  So it is tempting to think the bank, when aske...  ...  Mace Windu’s is purple because, the exhibition...\n",
              "2  That is not the view of the chief executive, J...  ...  (A Labor favourite) Why does it take so long t...\n",
              "3  Barclays thinks the DoJ’s claims are “disconne...  ...  Alternatively, you could buy books while suppo...\n",
              "4  But actually, some grudging respect for Staley...  ...  The 29-year-old actor plays a pickpocket who b...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xZi1Yn0T-UD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "710dd7e0-00ea-4ea7-c5c8-a0e3083dbd54"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>neg_1</th>\n",
              "      <th>neg_2</th>\n",
              "      <th>neg_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>183395</th>\n",
              "      <td>It feels as though Stone realised that some of...</td>\n",
              "      <td>Or you could say it was a battle for the soul ...</td>\n",
              "      <td>Newly sprouted flowers pushing up through the ...</td>\n",
              "      <td>23 mins: Peter Oh emails: “If the Black Cats a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183396</th>\n",
              "      <td>There are some fun elements, many involving Rh...</td>\n",
              "      <td>Goal!</td>\n",
              "      <td>The South Staffordshire and Shropshire trust h...</td>\n",
              "      <td>David Moyes has now had his pre-match chat wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183397</th>\n",
              "      <td>I particularly enjoyed a scene in which O’Bria...</td>\n",
              "      <td>Almost nine out of ten school leaders are tell...</td>\n",
              "      <td>“I haven’t supported Mr Trump at any point alo...</td>\n",
              "      <td>My patient taught me that we don’t have to act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183398</th>\n",
              "      <td>His carnivorous snarl fills the immense screen...</td>\n",
              "      <td>“Often the harm may be too small to make it pr...</td>\n",
              "      <td>Rather than privacy from the state, the real c...</td>\n",
              "      <td>She refused to comment on reports that Cameron...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183399</th>\n",
              "      <td>There’s a playful visual flair to this moment ...</td>\n",
              "      <td>“The MP for Dewsbury was employed by Virgin Ca...</td>\n",
              "      <td>(Like humans, they have sex for pleasure as we...</td>\n",
              "      <td>Man of the match Virgil van Dijk (Southampton)...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  ...                                              neg_3\n",
              "183395  It feels as though Stone realised that some of...  ...  23 mins: Peter Oh emails: “If the Black Cats a...\n",
              "183396  There are some fun elements, many involving Rh...  ...  David Moyes has now had his pre-match chat wit...\n",
              "183397  I particularly enjoyed a scene in which O’Bria...  ...  My patient taught me that we don’t have to act...\n",
              "183398  His carnivorous snarl fills the immense screen...  ...  She refused to comment on reports that Cameron...\n",
              "183399  There’s a playful visual flair to this moment ...  ...  Man of the match Virgil van Dijk (Southampton)...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbab41hoQzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('data.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07kqgnqzoOh5",
        "colab_type": "text"
      },
      "source": [
        "# TORCHTEXT and stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtGq1QulqO1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    return [tok.lemma_ for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuQh6VfuaqzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "1abc4dee-b219-42fc-bf08-8b751cb57101"
      },
      "source": [
        "# Using data to pretrain word-embeddings\n",
        "\n",
        "data_tokenized = list(df['text'].apply(lambda x: tokenize(x)))\n",
        "model = Word2Vec(data_tokenized, size=200, window=10, negative=5)  # building emb of size 200 (parameters from the paper)\n",
        "model_weights = torch.FloatTensor(model.wv.vectors)\n",
        "model.wv.save_word2vec_format('pretrained_embeddings')\n",
        "vectors = Vectors(name='pretrained_embeddings', cache='./')  # and saving the weights to build vocab later"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "  0%|          | 0/24240 [00:00<?, ?it/s]Skipping token b'24240' with 1-dimensional vector [b'200']; likely a header\n",
            " 96%|█████████▌| 23179/24240 [00:01<00:00, 13596.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcuDPIccohZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
        "\n",
        "TEXT = Field(sequential=True, \n",
        "             include_lengths=False, \n",
        "             batch_first=True, \n",
        "             tokenize=tokenize, \n",
        "             lower=True)\n",
        "\n",
        "RESULT = Field(sequential=True, \n",
        "             include_lengths=False, \n",
        "             batch_first=True, \n",
        "             tokenize=tokenize, \n",
        "             lower=True)\n",
        "\n",
        "dataset = TabularDataset(\n",
        "           path=\"/content/data.csv\",\n",
        "           format='csv',\n",
        "           skip_header=True,\n",
        "           fields=[('text', RESULT),('neg_1', TEXT), ('neg_2', TEXT), ('neg_3', TEXT)])\n",
        "\n",
        "TEXT.build_vocab(dataset, min_freq=2, vectors=vectors,\n",
        "                   unk_init = torch.Tensor.normal_)\n",
        "\n",
        "RESULT.build_vocab(dataset, min_freq=2, vectors=vectors,\n",
        "                   unk_init = torch.Tensor.normal_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBiZPau_tq94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = TEXT.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WDBH_AFtI8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9e76ae1b-3e1c-47ba-ecd8-e083c7dafc69"
      },
      "source": [
        "print('Vocab size:', len(TEXT.vocab.itos))\n",
        "TEXT.vocab.itos[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 52958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', 'the', 'be', 'a', 'to', 'of', 'and', 'in', 'that']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRU_7HrF-gxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = dataset.split(0.8)\n",
        "train, valid = train.split(0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKwWlNG1-NEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
        "    shuffle=True,\n",
        "    sort_key=lambda x: len(x.result),\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reQyCrXt_RLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8beb3876-86bc-414f-99f0-a83a62961da5"
      },
      "source": [
        "b = next(iter(train_iterator))\n",
        "vars(b).keys()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'neg_1', 'neg_2', 'neg_3'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYARfK2D_0h9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "929ef081-0ca6-48e3-9380-f5bf8bb96603"
      },
      "source": [
        "b.text.size(), b.neg_1.size(), b.neg_2.size(), b.neg_3.size()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 61]),\n",
              " torch.Size([64, 78]),\n",
              " torch.Size([64, 100]),\n",
              " torch.Size([64, 80]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LsY9Jr7zA1y",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHxS0RLJEiRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwBbIppuzB7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer = Adam\n",
        "\n",
        "# learning rate 0.001 for 15 epochs and batch size of 50. \n",
        "# We set the number of negative samples per input sample m to 20,\n",
        "# λ to 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6Du_eOEl0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, topics_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.topics_size = topics_size\n",
        "\n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        self.embeddings.weight.data.copy_(vocab.vectors)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.embed_size, self.topics_size)\n",
        "        self.fc2 = nn.Linear(self.topics_size, self.embed_size)   \n",
        "\n",
        "        self.bias = None\n",
        "\n",
        "\n",
        "    def sentence_embeddings(self, x):\n",
        "        '''\n",
        "        input: (batch_size, seq_length, embed_size)\n",
        "        output: (batch_size, embed_size)\n",
        "        '''\n",
        "        x = torch.sum(x, dim=1)/x.size()[1]\n",
        "        return x\n",
        "\n",
        "    \n",
        "    def forward(self, batch):\n",
        "        \n",
        "        text, neg_1, neg_2, neg_3 = batch.text, batch.neg_1, batch.neg_2, batch.neg_3\n",
        "\n",
        "        text_true = self.sentence_embeddings(self.embeddings(text))\n",
        "        neg_1 = self.sentence_embeddings(self.embeddings(neg_1))\n",
        "        neg_2 = self.sentence_embeddings(self.embeddings(neg_2))\n",
        "        neg_3 = self.sentence_embeddings(self.embeddings(neg_3))\n",
        "\n",
        "        \n",
        "        text_out = self.fc1(text_true)\n",
        "        text_out = F.softmax(text_out, dim=1)\n",
        "        text_out = self.fc2(text_out)\n",
        "\n",
        "        return text_true, text_out, neg_1, neg_2, neg_3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwtsQNuRuY2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ad3a0e84-3ee1-4200-ebcf-719eb65d7278"
      },
      "source": [
        "model = MyModel(vocab_size=len(vocab), embed_size=200, topics_size=10)\n",
        "model.to(device)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (embeddings): Embedding(52958, 200)\n",
              "  (fc1): Linear(in_features=200, out_features=10, bias=True)\n",
              "  (fc2): Linear(in_features=10, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdY_ERuudTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "d3c32827-fcba-40be-c792-34001b2386c9"
      },
      "source": [
        "model.forward(b)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1027, -0.0895,  0.0326,  ..., -0.0178, -0.0326, -0.1298],\n",
              "         [-0.0048, -0.0119,  0.0072,  ...,  0.0247, -0.0146, -0.0277],\n",
              "         [ 0.1114, -0.1159,  0.0220,  ...,  0.0841,  0.0010, -0.0294],\n",
              "         ...,\n",
              "         [-0.0439, -0.1392,  0.0732,  ...,  0.1561,  0.0902, -0.0220],\n",
              "         [-0.0087,  0.0303,  0.0064,  ...,  0.0101,  0.0033,  0.0102],\n",
              "         [ 0.0119,  0.0146,  0.0214,  ...,  0.0071,  0.0080, -0.0172]],\n",
              "        device='cuda:0', grad_fn=<DivBackward0>),\n",
              " tensor([[-0.0886,  0.1370, -0.1686,  ...,  0.1837, -0.2896, -0.1951],\n",
              "         [-0.0889,  0.1345, -0.1676,  ...,  0.1859, -0.2943, -0.1954],\n",
              "         [-0.0891,  0.1408, -0.1684,  ...,  0.1781, -0.2888, -0.1991],\n",
              "         ...,\n",
              "         [-0.0886,  0.1415, -0.1721,  ...,  0.1790, -0.2825, -0.1964],\n",
              "         [-0.0882,  0.1338, -0.1673,  ...,  0.1879, -0.2960, -0.1950],\n",
              "         [-0.0887,  0.1350, -0.1674,  ...,  0.1869, -0.2950, -0.1956]],\n",
              "        device='cuda:0', grad_fn=<AddmmBackward>),\n",
              " tensor([[ 0.0149, -0.0718, -0.0232,  ...,  0.0132, -0.0908, -0.2380],\n",
              "         [ 0.0674, -0.1476,  0.1287,  ...,  0.1014, -0.0230, -0.0355],\n",
              "         [ 0.0861, -0.0135, -0.1595,  ...,  0.1470,  0.1224,  0.0350],\n",
              "         ...,\n",
              "         [ 0.0858, -0.0843,  0.0094,  ...,  0.3169,  0.0937,  0.0443],\n",
              "         [ 0.0787, -0.1632, -0.0457,  ...,  0.0444,  0.0851, -0.0194],\n",
              "         [ 0.0547, -0.0182,  0.0045,  ...,  0.0185,  0.0082,  0.0133]],\n",
              "        device='cuda:0', grad_fn=<DivBackward0>),\n",
              " tensor([[ 0.0483, -0.0229, -0.0105,  ...,  0.0571, -0.0553, -0.0347],\n",
              "         [-0.0906,  0.0608,  0.0463,  ...,  0.2517, -0.0710, -0.2512],\n",
              "         [ 0.0401, -0.0944,  0.0041,  ...,  0.0491, -0.0249,  0.0156],\n",
              "         ...,\n",
              "         [ 0.0225, -0.0881,  0.0150,  ...,  0.0377,  0.0363,  0.0059],\n",
              "         [ 0.0469, -0.3356,  0.1979,  ...,  0.1428,  0.0275, -0.0577],\n",
              "         [-0.0189, -0.0045, -0.0306,  ...,  0.0824,  0.0018, -0.0137]],\n",
              "        device='cuda:0', grad_fn=<DivBackward0>),\n",
              " tensor([[ 0.0884, -0.0061, -0.1646,  ...,  0.1426, -0.0161, -0.0249],\n",
              "         [-0.0556,  0.0256, -0.0822,  ...,  0.0209,  0.0271, -0.0396],\n",
              "         [-0.1037,  0.2180, -0.1688,  ...,  0.5826, -0.0898,  0.1240],\n",
              "         ...,\n",
              "         [-0.1142, -0.0744,  0.0234,  ...,  0.1053, -0.1086, -0.0549],\n",
              "         [ 0.0572, -0.0641, -0.0620,  ...,  0.0606, -0.1096,  0.0041],\n",
              "         [-0.1064, -0.1267, -0.0747,  ...,  0.2821, -0.0344, -0.0847]],\n",
              "        device='cuda:0', grad_fn=<DivBackward0>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLsARbd7xDLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "cf151efb-ae53-40de-eefe-ec159d24fcef"
      },
      "source": [
        "t, t2, n1, n2, n3 = model.forward(b)\n",
        "\n",
        "t.size(), t2.size(), n1.size(), n2.size(), n3.size()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 200]),\n",
              " torch.Size([64, 200]),\n",
              " torch.Size([64, 200]),\n",
              " torch.Size([64, 200]),\n",
              " torch.Size([64, 200]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON3ye7y5F1a0",
        "colab_type": "text"
      },
      "source": [
        "**Training objective**:\n",
        "$$ J = \\sum_{s \\in D} \\sum_{i=1}^n max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
        "where   \n",
        "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
        "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
        "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
        "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdLN70N9yzbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LossFunction(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_true, emb_pred, negative, T, lambd):\n",
        "        super(LossFunction, self).__init__()\n",
        "\n",
        "        self.emb_true = emb_true\n",
        "        self.emb_pred = emb_pred\n",
        "        self.negative = negative\n",
        "        self.T = T\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def __call__(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLXUABwIBJFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(true, negative, pred, T, lambda):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiZDGCkqI578",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "def _train_epoch(model, iterator, optimizer, curr_epoch):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    n_batches = len(iterator)\n",
        "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss = loss.item()\n",
        "        \n",
        "        loss_smoothing = i / (i+1)\n",
        "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
        "\n",
        "        train_losses.append(running_loss)\n",
        "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
        "\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def _test_epoch(model, iterator):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    n_batches = len(iterator)\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            loss = model(batch)\n",
        "            epoch_loss += loss.data.item()\n",
        "            valid_losses.append(loss)\n",
        "\n",
        "    return epoch_loss / n_batches\n",
        "\n",
        "\n",
        "def nn_train(model, train_iterator, valid_iterator, optimizer, n_epochs=100,\n",
        "          scheduler=scheduler, early_stopping=0):\n",
        "\n",
        "    prev_loss = 100500\n",
        "    es_epochs = 0\n",
        "    best_epoch = None\n",
        "    history = pd.DataFrame()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = _train_epoch(model, train_iterator, optimizer, epoch)\n",
        "        valid_loss = _test_epoch(model, valid_iterator)\n",
        "        scheduler.step(valid_loss)\n",
        "\n",
        "        valid_loss = valid_loss\n",
        "        print('validation loss %.5f' % valid_loss)\n",
        "\n",
        "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
        "        history = history.append(record, ignore_index=True)\n",
        "\n",
        "        if early_stopping > 0:\n",
        "            if valid_loss > prev_loss:\n",
        "                es_epochs += 1\n",
        "            else:\n",
        "                es_epochs = 0\n",
        "\n",
        "            if es_epochs >= early_stopping:\n",
        "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
        "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
        "                break\n",
        "\n",
        "            prev_loss = min(prev_loss, valid_loss)\n",
        "\n",
        "# nn_train(model, train_loader, valid_loader, optimizer, n_epochs=100) # Удалила вывод, т.к. он длинный, график лосса ниже"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Wg8Gj_zCYa",
        "colab_type": "text"
      },
      "source": [
        "# Topic CoHerence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg9mNs1uzEza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}